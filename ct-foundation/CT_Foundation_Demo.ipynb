{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjl9khpJ8CoN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-health/imaging-research/blob/master/ct-foundation/CT_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-health/imaging-research/tree/master/ct-foundation\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2SSjj4fqzeOe"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnwIgD1bziUF"
      },
      "source": [
        "## CT Foundation API Demo\n",
        "The ipynb is a demonstration of using the\n",
        "[CT Foundation API](https://github.com/google-health/imaging-research/tree/master/ct-foundation)\n",
        "(this API computes embeddings for CT DICOMs).\n",
        "\n",
        "The contents include how to:\n",
        "\n",
        "-   Load the LIDC dataset from DICOMs stored in Google DICOM Store and labels stored in GCS\n",
        "-   Generate embeddings for the image files\n",
        "-   Train a small model using the embeddings\n",
        "\n",
        "**Note**: It can take some time to generate embeddings for thousands of images.\n",
        "For ease of use, by default, this colab uses precomputed embeddings. You can\n",
        "also calculate them from scratch again by updating the relevant param in the\n",
        "\"Global params\" section.\n",
        "\n",
        "### This notebook is for API demonstration purposes only\n",
        "\n",
        "**Note: This notebook is for API demonstration purposes only.**\n",
        "\n",
        "It's important to use evaluation datasets\n",
        "that reflect the expected distribution of images and patients you wish to use any downstream models on.\n",
        "\n",
        "This means that the best way to determine if this API is right for you is to try it with data that would be used for the downstream task you're interested in.\n",
        "\n",
        "**Note**: If you want to jump to training a model with embeddings, you can\n",
        "scroll down to [Train a model with the embeddings from NLST](#train-nlst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awoN_r8jOj0r"
      },
      "source": [
        "# Data Attribution\n",
        "\n",
        "This notebook makes use of two public datasets provided by the Cancer Imaging Archive which is managed by the United States  National Cancer Institute\n",
        "\n",
        "###  NLST Radiology CT Images CC BY 4.0\n",
        "[https://www.cancerimagingarchive.net/collection/nlst/](https://www.cancerimagingarchive.net/collection/nlst/)\n",
        "\n",
        "#### NLST Data Citation\n",
        " National Lung Screening Trial Research Team. (2013). Data from the National Lung Screening Trial (NLST) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.HMQ8-J677\n",
        "### LIDC-IDRI Data Access CC BY 3.0\n",
        "https://www.cancerimagingarchive.net/collection/lidc-idri/\n",
        "\n",
        "#### LIDC-IDRI Data Citation\n",
        "\n",
        "Armato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Van Beek, E. J. R., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C. , Qing, D. P. Y. , Roberts, R. Y., Smith, A. R., Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallam, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., Clarke, L. P. (2015). Data From LIDC-IDRI [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpU6d_WPXx8"
      },
      "source": [
        "# Installation & Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sfnWmzMCOwzG",
        "outputId": "5d507923-a1a0-4a0f-9f66-f1696065ed7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Collecting dicomweb-client[gcp]\n",
            "  Downloading dicomweb_client-0.59.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (2.32.3)\n",
            "Collecting retrying>=1.3.3 (from dicomweb-client[gcp])\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: Pillow>=8.3 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (11.0.0)\n",
            "Collecting pydicom>=2.2 (from dicomweb-client[gcp])\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying>=1.3.3->dicomweb-client[gcp]) (1.17.0)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading dicomweb_client-0.59.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: retrying, pydicom, dicomweb-client\n",
            "Successfully installed dicomweb-client-0.59.3 pydicom-3.0.1 retrying-1.3.4\n",
            "Collecting tf-models-official==2.14.0\n",
            "  Downloading tf_models_official-2.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (3.0.11)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (11.0.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.155.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.2.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.6.17)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.10.0.84)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.0.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (6.0.2)\n",
            "Collecting sacrebleu (from tf-models-official==2.14.0)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.2.0)\n",
            "Collecting seqeval (from tf-models-official==2.14.0)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.9.7)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.14.0 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow~=2.14.0 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (6.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.14.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.14.0) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.68.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.14.0) (2.17.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.14.0) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.14.0)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official==2.14.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.14.0) (1.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (1.13.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (1.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-models-official==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (5.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.14.0) (3.10)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.14.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.14.0) (3.5.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.1.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.14.0)\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.14.0) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.14.0) (1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official==2.14.0) (0.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.2.2)\n",
            "Downloading tf_models_official-2.14.0-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=dd28a7e0a2683dae0f3128e232c73755f822e3e69436cdcaf18fdd1e141d8f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: wrapt, tf-keras, tensorflow-model-optimization, tensorflow-estimator, portalocker, ml-dtypes, keras, colorama, sacrebleu, seqeval, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 portalocker-3.0.0 sacrebleu-2.4.3 seqeval-1.2.2 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.14.0 tf-keras-2.15.0 tf-models-official-2.14.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "# Notebook specific dependencies\n",
        "# !pip install google_health.ct_dicom\n",
        "# TODO OPTIONAL: Create pip installation for the code below - https://github.com/google-health/google-health/tree/master/ct_dicom\n",
        "\n",
        "!pip install absl-py dicomweb-client[gcp] google-auth requests-toolbelt\n",
        "!pip install tf-models-official==2.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RYMxVPMPzpR7",
        "outputId": "0c5ee029-d90c-4a59-de38-824643ae3059",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pydicom:get_frame_offsets is deprecated and will be removed in v4.0\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import http\n",
        "import matplotlib\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Iterable, Optional\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "import pandas as pd\n",
        "import dicomweb_client.ext.gcp.uri as gcp_uri\n",
        "import dicomweb_client.uri as dicomweb_uri\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "from google.auth import credentials as gcredentials\n",
        "from google.auth.transport import requests\n",
        "from requests_toolbelt.multipart import decoder\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_an_XYPdBA"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "7sY6uzinTGjg"
      },
      "outputs": [],
      "source": [
        "#@title Classes for testing data access and visualization\n",
        "\n",
        "\"\"\"Google Cloud Healthcare (CHC) DICOMweb utilities.\"\"\"\n",
        "\n",
        "# Well-known constants from https://www.dicomstandard.org/.\n",
        "_STUDY_INSTANCE_UID_TAG = '0020000D'\n",
        "_SERIES_INSTANCE_UID_TAG = '0020000E'\n",
        "_SOP_INSTANCE_UID_TAG = '00080018'\n",
        "\n",
        "_SERIES_INSTANCE_UID_SEARCH_SUFFIX = 'series'\n",
        "_STUDY_INSTANCE_UID_SEARCH_SUFFIX = 'studies'\n",
        "_SOP_INSTANCE_UID_SEARCH_SUFFIX = 'instances'\n",
        "\n",
        "_VALUE_KEY = 'Value'\n",
        "\n",
        "# Scope requirements from:\n",
        "# https://cloud.google.com/healthcare-api/docs/reference/rest/v1/projects.locations.datasets.dicomStores/searchForInstances#authorization-scopes\n",
        "_AUTHORIZATION_SCOPES = ['https://www.googleapis.com/auth/cloud-healthcare']\n",
        "\n",
        "# Search result limits for the CHC DICOMweb API:\n",
        "# https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "_MAX_LIMIT_STUDY = 5000\n",
        "_MAX_LIMIT_SERIES = 5000\n",
        "_MAX_LIMIT_SOP = 50000\n",
        "_MAX_OFFSET = 1000000\n",
        "\n",
        "_MAX_REFRESH_ATTEMPTS = 10\n",
        "_REQUEST_TIMEOUT_SECONDS = 600\n",
        "\n",
        "\n",
        "def create_authorized_session(\n",
        "    credentials: gcredentials.Credentials,\n",
        ") -> requests.AuthorizedSession:\n",
        "  \"\"\"Creates a Session authorized for Cloud Healthcare API interactions.\n",
        "\n",
        "  Args:\n",
        "    credentials: Google Auth credentials. For further details, see\n",
        "      https://googleapis.dev/python/google-auth/latest/index.html.\n",
        "\n",
        "  Returns:\n",
        "    Credentials object with the requisite API scope.\n",
        "  \"\"\"\n",
        "  authorization_scopes = _AUTHORIZATION_SCOPES\n",
        "  scoped_credentials = gcredentials.with_scopes_if_required(\n",
        "      credentials, authorization_scopes\n",
        "  )\n",
        "  return requests.AuthorizedSession(\n",
        "      scoped_credentials, max_refresh_attempts=_MAX_REFRESH_ATTEMPTS\n",
        "  )\n",
        "\n",
        "\n",
        "def download_multipart_dicom_series(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: str,\n",
        "    series_instance_uid: str,\n",
        ") -> Iterable[bytes]:\n",
        "  \"\"\"Downloads all SOP Instances (DICOMs) within a Series Instance UID.\n",
        "\n",
        "  The request accepts a multipart MIME response from the CHC DICOMweb API to\n",
        "  reduce the:\n",
        "  - Latency associated with making one API call per Instance.\n",
        "  - API quota usage while downloading all Instances within a Series.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: The Study Instance UID containing the Series Instance\n",
        "      UID to download.\n",
        "    series_instance_uid: The Series Instance UID containing the SOP Instances\n",
        "      (DICOMs) to download.\n",
        "\n",
        "  Yields:\n",
        "    DICOM bytes associated with each Instance contained within the input Series\n",
        "    Instance UID.\n",
        "  \"\"\"\n",
        "  dicomweb_path = str(\n",
        "      dicomweb_uri.URI(\n",
        "          str(\n",
        "              gcp_uri.GoogleCloudHealthcareURL(\n",
        "                  project_id, location, dataset_id, dicom_store_id\n",
        "              )\n",
        "          ),\n",
        "          study_instance_uid,\n",
        "          series_instance_uid,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  headers = {\n",
        "      'Accept': (\n",
        "          'multipart/related; transfer-syntax=1.2.840.10008.1.2.1;'\n",
        "          ' type=\"application/dicom\"'\n",
        "      )\n",
        "  }\n",
        "  response = session.get(\n",
        "      dicomweb_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "\n",
        "  for part in decoder.MultipartDecoder.from_response(response).parts:\n",
        "    yield part.content\n",
        "\n",
        "\n",
        "def search_study_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Study Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page of results\n",
        "      (one page per query) includes at most `limit` values. The higher this\n",
        "      value, the fewer the total number of requests, but each response would be\n",
        "      larger. Depending on your network connection, set this value in the range\n",
        "      1 through 5000 (both inclusive). This parameter impacts the speed and\n",
        "      network bandwidth utilization, but not the values returned by the method.\n",
        "\n",
        "  Yields:\n",
        "    Study Instance UIDs from the DICOM Store.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_STUDY:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_STUDY} for Study Instances.'\n",
        "    )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      _STUDY_INSTANCE_UID_SEARCH_SUFFIX,\n",
        "      _STUDY_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def search_series_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: Optional[str] = None,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Series Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  The scope may be restricted to all Series within a fixed Study Instance\n",
        "  UIDs (see `study_instance_uid` below).\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: If provided, restricts the returned Series Instance UIDs\n",
        "      to within this Study Instance UID.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page (query)\n",
        "      includes at most `limit` values. The higher this value, the fewer the\n",
        "      total number of requests, but each response would be larger. Depending on\n",
        "      your network connection, set this value in the range 1 through 5000 (both\n",
        "      inclusive).\n",
        "\n",
        "  Yields:\n",
        "    Series Instance UIDs from the DICOM Store (optionally within the scope of\n",
        "    the input `study_instance_uid`, if provided).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_SERIES:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_SERIES} for Series Instances.'\n",
        "    )\n",
        "  search_suffix = (\n",
        "      _SERIES_INSTANCE_UID_SEARCH_SUFFIX\n",
        "      if study_instance_uid is None\n",
        "      else f'studies/{study_instance_uid}/series'\n",
        "  )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      search_suffix,\n",
        "      _SERIES_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def _search_dicom_data(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    query_suffix: str,\n",
        "    dicom_tag: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Generates DICOM UIDs from a CHC DICOM Store.\"\"\"\n",
        "  assert limit > 0\n",
        "\n",
        "  uri = gcp_uri.GoogleCloudHealthcareURL(\n",
        "      project_id, location, dataset_id, dicom_store_id\n",
        "  )\n",
        "  base_dicomweb_query_path = f'{uri}/{query_suffix}?includefield={dicom_tag}'\n",
        "  headers = {'Content-Type': 'application/dicom+json; charset=utf-8'}\n",
        "\n",
        "  # The CHC offset limit puts an upper bound on the Instance count, which is\n",
        "  # also used to limit the number of iterations.\n",
        "  for offset in range(0, _MAX_OFFSET, limit):\n",
        "    dicomweb_query_path = (\n",
        "        f'{base_dicomweb_query_path}&offset={offset}&limit={limit}'\n",
        "    )\n",
        "\n",
        "    response = session.get(\n",
        "        dicomweb_query_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    # CHC DICOMweb API does not set a Warning response header on the last\n",
        "    # available page:\n",
        "    # https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "    if response.status_code == http.HTTPStatus.NO_CONTENT:\n",
        "      return\n",
        "\n",
        "    for instance in response.json():\n",
        "      assert dicom_tag in instance\n",
        "      assert _VALUE_KEY in instance[dicom_tag]\n",
        "\n",
        "      for value in instance[dicom_tag][_VALUE_KEY]:\n",
        "        yield value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eb82fhNmYF1P",
        "outputId": "0d6569be-a0fe-4fce-8595-75b7d968847f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ya29.a0ARW5m77YB3tEywXodz0GxvZbcS7r4V6PrSQvtxFIuQsg5quiTrbIILDPbQPRUTc5kqq5w1XOQlKCYX4K15-dXixYIjzJgBA1iyrDIhllnLvOSCtWIvj88FT3UZtA3lpFSqv7aPW7cS2IyV4qj-VOm16qHKtTJ4dbCJOdLIDYaCgYKAXESARASFQHGX2Mi2PXEexp6nrkoTLBTx-v3ww0175'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# @title Authenticate\n",
        "# Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "auth.authenticate_user()\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "# This is your token for accessing the API and CT Volumes.\n",
        "# It's good for 1 hour until you need a new one.\n",
        "TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lfmFZPCFM2G2"
      },
      "outputs": [],
      "source": [
        "# @title Set DICOM store parameters\n",
        "project_id='ct-foundation-445901' # @param {type:\"string\"}\n",
        "location='us-central1'  # @param {type:\"string\"}\n",
        "dataset_id='headct-example-20241226'  # @param {type:\"string\"}\n",
        "dicom_store_id='headct_example_20241226' # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQRxrXiUUgv"
      },
      "source": [
        "<a name=\"train-nlst\"></a>\n",
        "# Train a model with the embeddings from NLST\n",
        "\n",
        "Here we have a full set of embeddings from the NLST dataset that you can download and train a cancer detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkB5R0sx7C7Z"
      },
      "source": [
        "## Collect the stored NPZ data from the cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzI0A92hH5VZ"
      },
      "outputs": [],
      "source": [
        "# GCS bucket with data to read:\n",
        "gcs_storage_client = storage.Client(project_id) # our bucket is in the same Google Cloud Project as the DicomStore\n",
        "gcs_bucket_name = 'hai-cd3-foundations-ct3d-vault-entry'# @param {type:\"string\"}\n",
        "gcs_bucket = gcs_storage_client.bucket(gcs_bucket_name)\n",
        "tune_path = 'nlst/nlst_tune_with_labels.npz' # @param {type:\"string\"}\n",
        "train_path = 'nlst/nlst_train_with_labels.npz' # @param {type:\"string\"}\n",
        "\n",
        "def read_embeddings(path):\n",
        "  blob = gcs_bucket.blob(path)\n",
        "  with blob.open('rb') as f:\n",
        "    data = np.load(f,allow_pickle=True)\n",
        "    key = data.files[0]\n",
        "    return pd.DataFrame.from_dict(data[key].item(), orient='index')\n",
        "\n",
        "df_tune = read_embeddings(tune_path)\n",
        "df_train = read_embeddings(train_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o15b23RR7WYz"
      },
      "source": [
        "## Train and Evaluate a model using precomputed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1cEUPaH90DV"
      },
      "outputs": [],
      "source": [
        "# choose whether you want predict whether the screen leads to a positive lung cancer diagnosis within 1 or 2 years.\n",
        "model_head = 'cancer_in_2' # @param [\"cancer_in_1\",\"cancer_in_2\"]\n",
        "# Get NumPy arrays from DataFrames\n",
        "train_embeddings = df_train.embedding\n",
        "train_labels = df_train[model_head].values\n",
        "tune_embeddings = df_tune.embedding\n",
        "tune_labels = df_tune[model_head].values\n",
        "\n",
        "# Convert the NumPy arrays to ragged tensors\n",
        "train_embeddings = tf.constant(list(train_embeddings))\n",
        "tune_embeddings = tf.convert_to_tensor(list(tune_embeddings))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings, train_labels))\n",
        "eval_ds = tf.data.Dataset.from_tensor_slices((tune_embeddings, tune_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnAmHU5mA88c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 token_num=1,\n",
        "                 embeddings_size=1408,\n",
        "                 learning_rate=0.07,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.5,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[128, 32],\n",
        "                 weight_decay=0.0001,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS.\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(token_num * embeddings_size,))\n",
        "  inputs_reshape = tf.keras.layers.Reshape((token_num, embeddings_size))(inputs)\n",
        "  inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(inputs_reshape)\n",
        "  hidden = inputs_pooled\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "         learning_rate=.1),\n",
        "      loss=dict([(head, 'binary_focal_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or 1.0,\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZIUOHpWEyj5"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "DIAGNOSIS = 'cancer_in_2'\n",
        "model = create_model(\n",
        "    [DIAGNOSIS]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=train_ds.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=eval_ds.batch(32).cache(),\n",
        "    epochs=35,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh24XhCaE15V"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-n_8DvClC9Z"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in eval_ds.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULHjSMx8lFyY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_McpwVQRCV"
      },
      "source": [
        "# Test access to LIDC DICOM store and the API\n",
        "\n",
        "Get a token that grants access to the DICOM store and use it to download a volume via the DICOMWEb API. Next, we can collect the embeddings from the\n",
        "selected embedding.\n",
        "\n",
        "**NOTE**: You can skip this section if you just want to train a model on NLST data using the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMANNgm4crh"
      },
      "source": [
        "## Download a CT volume to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6VOuJFzaXlWe"
      },
      "outputs": [],
      "source": [
        "# @title Create a Session via a token.\n",
        "creds = credentials.Credentials(TOKEN)\n",
        "session = create_authorized_session(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0qKyk_7ZLfK",
        "outputId": "95fd5f8f-585e-410f-890b-0afc8f1399d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Slices in downloaded volume: 65\n"
          ]
        }
      ],
      "source": [
        "# @title List the study and series instance UIDs and download a single volume.\n",
        "TOTAL_VOLUMES = 5  # 列出的总数\n",
        "VOLUME_TO_SHOW = 1  # 要渲染显示的体积编号\n",
        "\n",
        "# 搜索study UIDs\n",
        "study_uids = list(search_study_instance_uids(\n",
        "    project_id='ct-foundation-445901',\n",
        "    location='us-central1',\n",
        "    dataset_id='headct-example-20241226',\n",
        "    dicom_store_id='headct_example_20241226',\n",
        "    session=session))\n",
        "\n",
        "# 获取对应的series UIDs\n",
        "corresponding_series_uids = []\n",
        "for study_number, a_study_uid in enumerate(study_uids):\n",
        "    a_series = list(search_series_instance_uids(\n",
        "        project_id='ct-foundation-445901',\n",
        "        location='us-central1',\n",
        "        dataset_id='headct-example-20241226',\n",
        "        dicom_store_id='headct_example_20241226',\n",
        "        session=session,\n",
        "        study_instance_uid=a_study_uid))[0]\n",
        "    corresponding_series_uids.append(a_series)\n",
        "    if study_number == TOTAL_VOLUMES:\n",
        "        break\n",
        "\n",
        "# 下载指定的volume\n",
        "volume_as_bytes = list(download_multipart_dicom_series(\n",
        "    project_id='ct-foundation-445901',\n",
        "    location='us-central1',\n",
        "    dataset_id='headct-example-20241226',\n",
        "    dicom_store_id='headct_example_20241226',\n",
        "    session=session,\n",
        "    study_instance_uid=study_uids[VOLUME_TO_SHOW],\n",
        "    series_instance_uid=corresponding_series_uids[VOLUME_TO_SHOW],\n",
        "))\n",
        "\n",
        "print(f'Total Slices in downloaded volume: {len(volume_as_bytes)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJUE96Qh0ZQY",
        "outputId": "a81d5317-cb06-46f4-9916-b540c02cb103"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 607)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#@title Render a single slice\n",
        "SLICE_TO_RENDER = 15\n",
        "example_dicom = pydicom.dcmread(io.BytesIO(volume_as_bytes[SLICE_TO_RENDER]))\n",
        "\n",
        "arr_unsigned = example_dicom.pixel_array.copy()\n",
        "arr_unsigned = arr_unsigned.astype(np.float32)\n",
        "arr_unsigned.shape\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPOseIHV4lAF"
      },
      "source": [
        "## Call the API to compute embeddings for the selected volume.\n",
        "\n",
        "**NOTE:** *The API can take up to 10 minutes to scale individual instances. If you get errors, wait and attempt them again.*\n",
        "\n",
        "Errors results in a FAIL Status string instead of embeddings in the returned list.\n",
        "\n",
        "**NOTE:** Up to 300 parallel requests can be made if the system is fully scaled. Please start at 50 and reduce requests if you are getting end point errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzal615K4rDv"
      },
      "outputs": [],
      "source": [
        "#@title Python methods to call CT Foundation's API.\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import dataclasses\n",
        "import functools\n",
        "import json\n",
        "from typing import Any, Tuple\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(eq=False, frozen=True)\n",
        "class Response:\n",
        "  \"\"\"Response from a Vertex Endpoint.\"\"\"\n",
        "\n",
        "  status_code: int\n",
        "  response_json: dict[str, Any] | None  # json_types.JSONObject\n",
        "\n",
        "\n",
        "class Endpoint:\n",
        "  \"\"\"Calling utility for a Vertex Endpoint using default credentials.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._endpoint_url = (\n",
        "        'https://us-central1-aiplatform.googleapis.com/v1/projects/'\n",
        "        'hai-cd3-foundations/locations/us-central1/endpoints/300')\n",
        "\n",
        "  def predict(\n",
        "      self,\n",
        "      instances=list[Any],\n",
        "      parameters: dict[str, Any] | None = None,\n",
        "      credentials: google.auth.credentials.Credentials | None = None,\n",
        "  ) -> Response:\n",
        "    \"\"\"Calls the Vertex Endpoint with the given instances and parameters.\"\"\"\n",
        "    if credentials is None:\n",
        "      credentials = google.auth.default()[0]\n",
        "    session = google.auth.transport.requests.AuthorizedSession(\n",
        "        credentials=credentials\n",
        "    )\n",
        "    response = session.post(\n",
        "        self._endpoint_url + ':predict',\n",
        "        json=(\n",
        "            {'instances': instances}\n",
        "            | ({'parameters': parameters} if parameters is not None else {})\n",
        "        ),\n",
        "        headers={\n",
        "            'Content-Type': 'application/json',\n",
        "        },\n",
        "        timeout=400\n",
        "    )\n",
        "    try:\n",
        "      response_json = response.json()\n",
        "    except json.JSONDecodeError:\n",
        "      # Not expected, handling in case server incorrectly returns non-JSON.\n",
        "      response_json = None\n",
        "    return Response(\n",
        "        status_code=response.status_code,\n",
        "        response_json=response_json,\n",
        "    )\n",
        "\n",
        "\n",
        "def call_single_batch(\n",
        "    caller: Endpoint,\n",
        "    credentials,\n",
        "    urls: list[str],\n",
        "    access_token: str\n",
        ") -> list[Tuple[np.ndarray | str, str]]:\n",
        "  \"\"\"Handles calls for a single batch and returns embeddings.\"\"\"\n",
        "  return_data = []\n",
        "  if not credentials.valid:\n",
        "    credentials.refresh(google.auth.transport.requests.Request())\n",
        "  instances = [{\n",
        "      \"dicom_path\": a_url, \"bearer_token\": f\"{access_token}\"} for a_url in urls]\n",
        "  returns = caller.predict(instances=instances)\n",
        "  if returns.status_code != 200:\n",
        "    for a_url in urls:\n",
        "      return_data.append((f'FAIL STATUS {returns.status_code}', a_url))\n",
        "    return return_data\n",
        "  else:\n",
        "    for i in range(len(returns.response_json['predictions'])):\n",
        "      if returns.response_json['predictions'][i]['error_response']:\n",
        "        return_data.append((\n",
        "            returns.response_json['predictions'][i]['error_response'], urls[i]))\n",
        "      else:\n",
        "        embeddings = returns.response_json['predictions'][i][\n",
        "            'embedding_result'\n",
        "        ]['embedding']\n",
        "        return_data.append((embeddings, urls[i]))\n",
        "    return return_data\n",
        "\n",
        "\n",
        "def get_ct_embbeddings(\n",
        "    caller: Endpoint,\n",
        "    credentials,\n",
        "    urls: list[str],\n",
        "    access_token: str,\n",
        "    batch_size: int,\n",
        "    parallel_size: int,\n",
        ") -> list[Tuple[np.ndarray | str, str]]:\n",
        "  \"\"\"Handles calls and returns for parallel requests.\n",
        "\n",
        "  Args:\n",
        "    caller: CT foundation API caller.\n",
        "    credentials: The credentials for the API.\n",
        "    urls: List of urls to the DICOM store for series to run.\n",
        "      This must be of length batch_size * parallel_size.\n",
        "    access_token: Access token for the DICOM store.\n",
        "    batch_size: The number of volumes to pass in a batch (max 5).\n",
        "    parallel_size: The number of parallel calls.\n",
        "\n",
        "  Returns:\n",
        "    Tuple list of embeddings | errors and the corresponding urls from which\n",
        "      the embeddings were computed.\n",
        "  \"\"\"\n",
        "  assert batch_size < 6, 'Batch size must be 5 or less.'\n",
        "  assert (\n",
        "      len(urls) == batch_size * parallel_size\n",
        "  ), 'Error in batch, parallel sizes versus requests'\n",
        "\n",
        "  # Setup up parallel batches\n",
        "  p_urls = []\n",
        "  for i in range(parallel_size):\n",
        "    p_urls.append(urls[i * batch_size : (i + 1) * batch_size])\n",
        "\n",
        "  # Check for correct sizing\n",
        "  assert len(p_urls) == parallel_size, 'Error in batch, parallel dimensions'\n",
        "\n",
        "  call_batch = functools.partial(call_single_batch, caller, credentials)\n",
        "\n",
        "  # Launch parallel calls\n",
        "  with ThreadPoolExecutor(max_workers=parallel_size) as executor:\n",
        "    futures = [\n",
        "        executor.submit(call_batch, b_urls, access_token) for b_urls in p_urls\n",
        "    ]\n",
        "    results = [f.result() for f in futures]\n",
        "  # Unpack results into a single list\n",
        "  return_results = []\n",
        "  for b_result in results:\n",
        "    for a_result in b_result:\n",
        "      return_results.append(a_result)\n",
        "  return return_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_YHl2E30nHo"
      },
      "outputs": [],
      "source": [
        "#@title Create a URL, token, and call the API for the DICOM volume\n",
        "\n",
        "def create_lidc_series_url(study_instance_uid, series_instance_uid):\n",
        "  \"\"\"Create a URL to the specific LIDC DICOM volume.\"\"\"\n",
        "  return ('https://healthcare.googleapis.com/v1/projects/hai-cd3-foundations/'\n",
        "          'locations/us-central1/datasets/ct3d/dicomStores/lidc-idri/dicomWeb/'\n",
        "          f'studies/{study_instance_uid}/series/{series_instance_uid}')\n",
        "\n",
        "# Credentials to access the API\n",
        "credentials = google.auth.default()[0]\n",
        "\n",
        "# Token to access the DICOMs in the DICOM store\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "# The url pointing to the specific DICOM series that can be accessed via\n",
        "# the above token.\n",
        "my_url = create_lidc_series_url(study_uids[VOLUME_TO_SHOW],\n",
        "                                corresponding_series_uids[VOLUME_TO_SHOW])\n",
        "\n",
        "# Call the API with a single call and a batch size of 1.\n",
        "my_embeddings = get_ct_embbeddings(\n",
        "    caller=Endpoint(), credentials=credentials, urls=[my_url],\n",
        "    access_token=TOKEN, batch_size=1, parallel_size=1)\n",
        "# Total passed urls are 1\n",
        "print(f'Embeddings or error message for the CT in the DICOM store at: {my_embeddings[0][1]}')\n",
        "print(my_embeddings[0][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cx9ZvHRysS3"
      },
      "source": [
        "# Trying CT Foundation on your own CT DICOMs\n",
        "\n",
        "\n",
        "1.   [Create your own DICOM store](https://cloud.google.com/healthcare-api/docs/)\n",
        "2.   Upload your DICOMs to the store.\n",
        "3.   Call the API for a given study / series in your DICOM store.\n",
        "4.   Collect and store your embeddings for training.\n",
        "\n",
        "**NOTE**: If performing parallel calls, i.e. parallel_size >1, please start at\n",
        "50 or less as a start.\n",
        "\n",
        "If you have any feedback or questions please email us at: ct-foundation@google.com\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KTpU6d_WPXx8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}